<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Language Models Visualised</title>
    <script src="https://cdn.plot.ly/plotly-3.0.1.min.js" charset="utf-8"></script>
    <script defer type="module" src="plot.js"></script>
    <link rel="stylesheet" href="style.css">
</head>



<body>
    <main>
    <section>
        <h1>Language Models Visualised</h1>
        <h3>An AI-assisted experiment</h3>
        <p>The goal of this experiment was to find out if words from a language model can be visualised in 3D, while also learning about various related algorithms and models. All of this using AI code generation tools. <a href="#footer">See the footer for some prompting tips!</a></p>
        <h3>Words as vectors</h3>
        <p>A language model works by converting text into vectordata (a list of numbers), which helps the model find relations between words and sentences. Vectordata for the word "cat" might look like this: <pre>[0.47685,-0.084552,1.4641,0.047017,0.14686,0.5082,-1.2228,-0.22607,0.19306,-0.29756,...etc]</pre></p>
        <p>If we can somehow reduce this array of numbers to 3, we can place the words in a 3D space ⤵️</p>
    </section>
    <section class="graph">
        <div id="plot"></div>
    </section>
    <section>
        <h3>Too many dimensions</h3>
        <p>Depending on the language model you use, the amount of numbers (dimensions) in a vector can be quite large. <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI's ADA</a> model will generate 1536 numbers, and <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">HuggingFace MiniLM-L6</a> will generate 384 numbers for one prompt (a word or a sentence)</p>
        <p>Reducing these vectors down to 3 proved quite challenging. The problem is that advanced LLM's do not really work by assigning vectors to single words. These models are very good at finding complex relationships in complete narratives and sentences. Also, single words can consist of multiple tokens.</p>

        <h4>Comparing relations</h4>
        <p>One thing that does work with vectordata of these complex LLM's is to calculate the distance between two prompts, or sentences. Distance can be calculated as <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine</a> (angles) or <a href="https://en.wikipedia.org/wiki/Euclidean_distance">euclidian</a> (pythagorean distance). This graphic shows how close <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">MiniLM-V6</a> sentences are to each other.</p>
<pre>
My garden is full of flowers <->        Flowers and plants are in my garden 
My garden is full of flowers <->        My garden is very beautiful         
My garden is full of flowers <->        Red flowers are my favorite 
The race track has many cars <------>   Shakespeare couldn't write a play in a day
The race track has many cars <------->  Shakespeare wrote many plays 
The race track has many cars <--------> Shakespeare wrote plays about love 
</pre>
<p>HuggingFace models can be loaded locally using <a href="https://huggingface.co/docs/hub/en/transformers-js">tranformers.js</a></p>
    </section>
    <section>
        <h3>A simpler model</h3>
        <p>To get a better result for 3D visualisation it proved easier to use an old-fashioned language model, that gives you a static vector array for each word. This is how natural language processing worked before the age of LLM's.</p>
        <p><a href="https://nlp.stanford.edu/projects/glove/">The Glove Model</a> is a simple text file that contains 50 vectors for common english words. Using this model, we can use the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">distance</a> between these vectors to find related words.
<pre>
findRelatedWords("cat")
[
    { word: 'dog',    similarity: 0.9218 },
    { word: 'rabbit', similarity: 0.8487 },
    { word: 'monkey', similarity: 0.8041 },
    { word: 'rat',    similarity: 0.7891 },
    { word: 'cats',   similarity: 0.7865 }
]</pre></p>
            <h4>Reducing dimensions</h4>
            <p>To reduce these 50 dimensions to 3 we need a dimension reduction algorithm. ChatGPT suggested the <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">T-SNE</a> and the <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> algorithms.<br>Experimenting proved that this <a href="https://github.com/mljs/pca">Javascript PCA library</a> is able to reduce 50 dimensions to 3, without losing the relative positions of the words. This allows us to create the <a href="https://plotly.com/javascript/">3D plotly graph</a> on the top of this page!</p>
<pre>
"word": "car",
"vector": [
    0.49308144754274674,
    -3.650149578055416,
    1.5510818764700405
]
</pre>
        </section>
        <section class="footer" id="footer">
            <h3>Vibe Coding</h3>
            <p>
                A lot of this research was accelerated tremendously by using AI code generation such as Claude, Copilot and Blackbox. Here are a couple of prompts that worked well for me:</p>
<pre>
- How can I load a Huggingface model with transformers.js?
- Which language models are good at generating simple vectors for single words?
- Give a code example for reading the GLOVE language model
- How can I calculate the distance between 50-dimensional vectors?
- How can I reduce a vector of 50 dimensions to 3?
- Create a 3D plotly graph that shows my vector data
</pre>
The main concept of this experiment, and also this explanation, was created without any AI assistance.
        </section>
    </main>
   
</body>

</html>